{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto\n",
    "* Karen Fuentes\n",
    "* Marco Ramirez\n",
    "* Jenifer Arce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/marcoramirez/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/marcoramirez/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from xgboost import XGBClassifier\n",
    " # Instalación de librerias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import re, string, unicodedata\n",
    "import contractions\n",
    "import inflect\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = nltk.corpus.stopwords.words('spanish')\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "ps = PorterStemmer()\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainData():\n",
    "    df = pd.read_csv('fake_news_spanish.csv',\n",
    "                    encoding='utf-8', sep=\";\")\n",
    "    return df\n",
    "\n",
    "def getTestData():\n",
    "    data_test = pd.read_csv(\n",
    "        'fake_news_test.csv', encoding='utf-8', sep=\";\")\n",
    "    return data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data spanish (57063, 5)\n",
      "data test (1000, 4)\n"
     ]
    }
   ],
   "source": [
    "print('data spanish', getTrainData().shape)\n",
    "print('data test', getTestData().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID             object\n",
       "Label           int64\n",
       "Titulo         object\n",
       "Descripcion    object\n",
       "Fecha          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData = getTrainData()\n",
    "trainData.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Label</th>\n",
       "      <th>Titulo</th>\n",
       "      <th>Descripcion</th>\n",
       "      <th>Fecha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID</td>\n",
       "      <td>1</td>\n",
       "      <td>'The Guardian' va con Sánchez: 'Europa necesit...</td>\n",
       "      <td>El diario británico publicó este pasado jueves...</td>\n",
       "      <td>02/06/2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID</td>\n",
       "      <td>0</td>\n",
       "      <td>REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACIÓN ...</td>\n",
       "      <td>REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACIÓN ...</td>\n",
       "      <td>01/10/2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID</td>\n",
       "      <td>1</td>\n",
       "      <td>El 'Ahora o nunca' de Joan Fuster sobre el est...</td>\n",
       "      <td>El valencianismo convoca en Castelló su fiesta...</td>\n",
       "      <td>25/04/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID</td>\n",
       "      <td>1</td>\n",
       "      <td>Iglesias alienta a Yolanda Díaz, ERC y EH Bild...</td>\n",
       "      <td>En política, igual que hay que negociar con lo...</td>\n",
       "      <td>03/01/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID</td>\n",
       "      <td>0</td>\n",
       "      <td>Puigdemont: 'No sería ninguna tragedia una rep...</td>\n",
       "      <td>En una entrevista en El Punt Avui, el líder de...</td>\n",
       "      <td>09/03/2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ID</td>\n",
       "      <td>1</td>\n",
       "      <td>El PNV consolida su mayoría, el PSE salva los ...</td>\n",
       "      <td>Los nacionalistas consiguen las alcaldías de B...</td>\n",
       "      <td>26/05/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ID</td>\n",
       "      <td>0</td>\n",
       "      <td>El exconsejero Núria Marín pide el indulto en ...</td>\n",
       "      <td>Sus familiares aluden a su honestidad e integr...</td>\n",
       "      <td>16/09/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ID</td>\n",
       "      <td>1</td>\n",
       "      <td>La Fiscalía pide prisión incondicional para lo...</td>\n",
       "      <td>Suprime el delito de rebelión que les imputó i...</td>\n",
       "      <td>26/09/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ID</td>\n",
       "      <td>1</td>\n",
       "      <td>José Manuel Pérez Tornero, el creador de la te...</td>\n",
       "      <td>El futuro presidente de RTVE es licenciado en ...</td>\n",
       "      <td>25/02/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ID</td>\n",
       "      <td>0</td>\n",
       "      <td>La 'Ayusización' del BNG: Santiago Abascal ins...</td>\n",
       "      <td>Pablo Santiago Abascal planea vivir de las ren...</td>\n",
       "      <td>10/05/2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Label                                             Titulo  \\\n",
       "0  ID      1  'The Guardian' va con Sánchez: 'Europa necesit...   \n",
       "1  ID      0  REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACIÓN ...   \n",
       "2  ID      1  El 'Ahora o nunca' de Joan Fuster sobre el est...   \n",
       "3  ID      1  Iglesias alienta a Yolanda Díaz, ERC y EH Bild...   \n",
       "4  ID      0  Puigdemont: 'No sería ninguna tragedia una rep...   \n",
       "5  ID      1  El PNV consolida su mayoría, el PSE salva los ...   \n",
       "6  ID      0  El exconsejero Núria Marín pide el indulto en ...   \n",
       "7  ID      1  La Fiscalía pide prisión incondicional para lo...   \n",
       "8  ID      1  José Manuel Pérez Tornero, el creador de la te...   \n",
       "9  ID      0  La 'Ayusización' del BNG: Santiago Abascal ins...   \n",
       "\n",
       "                                         Descripcion       Fecha  \n",
       "0  El diario británico publicó este pasado jueves...  02/06/2023  \n",
       "1  REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACIÓN ...  01/10/2023  \n",
       "2  El valencianismo convoca en Castelló su fiesta...  25/04/2022  \n",
       "3  En política, igual que hay que negociar con lo...  03/01/2022  \n",
       "4  En una entrevista en El Punt Avui, el líder de...  09/03/2018  \n",
       "5  Los nacionalistas consiguen las alcaldías de B...  26/05/2019  \n",
       "6  Sus familiares aluden a su honestidad e integr...  16/09/2022  \n",
       "7  Suprime el delito de rebelión que les imputó i...  26/09/2019  \n",
       "8  El futuro presidente de RTVE es licenciado en ...  25/02/2021  \n",
       "9  Pablo Santiago Abascal planea vivir de las ren...  10/05/2021  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Titulo</th>\n",
       "      <th>Descripcion</th>\n",
       "      <th>Fecha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID</td>\n",
       "      <td>La mesa del congreso censura un encuentro inte...</td>\n",
       "      <td>Portavoces de Ciudadanos, PNV, UPN, PSOE, Unid...</td>\n",
       "      <td>30/10/2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID</td>\n",
       "      <td>La brecha digital que dificulta el acceso de a...</td>\n",
       "      <td>No es la primera vez que los ciudadanos vulner...</td>\n",
       "      <td>15/03/2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID</td>\n",
       "      <td>PP apremia al EQUO a presentar una propuesta d...</td>\n",
       "      <td>El partido morado reprocha que los socialistas...</td>\n",
       "      <td>01/07/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID</td>\n",
       "      <td>De soberano garante de la democracia a rey cor...</td>\n",
       "      <td>La renuncia de Felipe VI a su herencia, proced...</td>\n",
       "      <td>16/03/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID</td>\n",
       "      <td>El Gobierno aprobará este martes detraer los b...</td>\n",
       "      <td>El Ejecutivo también prorrogará la suspensión ...</td>\n",
       "      <td>13/09/2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                             Titulo  \\\n",
       "0  ID  La mesa del congreso censura un encuentro inte...   \n",
       "1  ID  La brecha digital que dificulta el acceso de a...   \n",
       "2  ID  PP apremia al EQUO a presentar una propuesta d...   \n",
       "3  ID  De soberano garante de la democracia a rey cor...   \n",
       "4  ID  El Gobierno aprobará este martes detraer los b...   \n",
       "\n",
       "                                         Descripcion       Fecha  \n",
       "0  Portavoces de Ciudadanos, PNV, UPN, PSOE, Unid...  30/10/2018  \n",
       "1  No es la primera vez que los ciudadanos vulner...  15/03/2023  \n",
       "2  El partido morado reprocha que los socialistas...  01/07/2019  \n",
       "3  La renuncia de Felipe VI a su herencia, proced...  16/03/2020  \n",
       "4  El Ejecutivo también prorrogará la suspensión ...  13/09/2021  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getTestData().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   ID           1000 non-null   object\n",
      " 1   Titulo       999 non-null    object\n",
      " 2   Descripcion  1000 non-null   object\n",
      " 3   Fecha        1000 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 31.4+ KB\n"
     ]
    }
   ],
   "source": [
    "testData = getTestData()\n",
    "testData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "1    33158\n",
      "0    23905\n",
      "Name: count, dtype: int64\n",
      "========== datos en nan ==========\n",
      "0\n",
      "0\n",
      "16\n",
      "0\n",
      "0\n",
      "========== Distribución labels ==========\n",
      "label 1: 0.58 %\n",
      "label 0: 0.42 %\n"
     ]
    }
   ],
   "source": [
    "trainData = getTrainData()\n",
    "print(trainData['Label'].value_counts())\n",
    "\n",
    "print('='*10, 'datos en nan', '='*10)\n",
    "\n",
    "print(trainData[\"ID\"].isna().sum())\n",
    "print(trainData[\"Label\"].isna().sum())\n",
    "print(trainData[\"Titulo\"].isna().sum())\n",
    "print(trainData[\"Descripcion\"].isna().sum())\n",
    "print(trainData[\"Fecha\"].isna().sum())\n",
    "trainData = trainData[trainData[\"Titulo\"].notna()]\n",
    "print('='*10, 'Distribución labels', '='*10)\n",
    "print('label 1:',round(trainData[trainData['Label'] == 1].shape[0]/trainData.shape[0],2),'%')\n",
    "print('label 0:',round(trainData[trainData['Label'] == 0].shape[0]/trainData.shape[0],2),'%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(trainData[\"ID\"].isna().sum())\n",
    "print(trainData[\"Label\"].isna().sum())\n",
    "print(trainData[\"Titulo\"].isna().sum())\n",
    "print(trainData[\"Descripcion\"].isna().sum())\n",
    "print(trainData[\"Fecha\"].isna().sum())\n",
    "\n",
    "trainData = trainData[trainData[\"Titulo\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from num2words import num2words\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word is not None:\n",
    "          new_word = unicodedata.normalize('NFKD', word).encode(\n",
    "              'ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "          new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = text.replace('á', 'a')\n",
    "    text = text.replace('é', 'e')\n",
    "    text = text.replace('í', 'i')\n",
    "    text = text.replace('ó', 'o')\n",
    "    text = text.replace('ú', 'u')\n",
    "    text = text.replace('ü', 'u')\n",
    "    text = text.replace('ñ', 'n')\n",
    "\n",
    "    # Eliminate punctuation and special characters by replacing them\n",
    "    text = text.replace('(', '').replace(')', '')\n",
    "    text = text.replace('[', '').replace(']', '')\n",
    "    text = text.replace('{', '').replace('}', '')\n",
    "    text = text.replace('<', '').replace('>', '')\n",
    "    return text.lower()\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    return [normalize_text(word) for word in words]\n",
    "\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word is not None:\n",
    "            new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "            if new_word != '':\n",
    "                new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "   \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "   new_words = []\n",
    "   for word in words:\n",
    "       if word.isdigit():\n",
    "           new_word = num2words(word, lang='es')\n",
    "           new_words.append(new_word)\n",
    "       else:\n",
    "           new_words.append(word)\n",
    "   return new_words\n",
    "\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    nuevas_palabras = []\n",
    "    for palabra in words:\n",
    "        if palabra is not None:\n",
    "            if palabra not in stopwords.words('spanish'):\n",
    "                nuevas_palabras.append(palabra)\n",
    "    return nuevas_palabras\n",
    "\n",
    "\n",
    "def corregir_contracciones_espanol(texto):\n",
    "    texto = texto.replace('al ', 'a el ').replace(\n",
    "        'al.', 'a el.')  # \"al\" a \"a el\"\n",
    "    # \"del\" a \"de el\"    # Agrega más reglas aquí según sea necesario\n",
    "    texto = texto.replace('del ', 'de el ').replace('del.', 'de el.')\n",
    "    return texto\n",
    "\n",
    "def preprocessing(words):\n",
    "    words = words.split()\n",
    "    words = to_lowercase(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_non_ascii(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = SnowballStemmer(\"spanish\")\n",
    "    return [stemmer.stem(word) for word in words]\n",
    "\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Simple lemmatization for verbs in list of tokenized words\"\"\"\n",
    "    # Simple rule-based lemmatization (you can extend this as needed)\n",
    "    lemmatized_words = []\n",
    "    for word in words:\n",
    "        if word.endswith('ar'):\n",
    "            lemmatized_words.append(word[:-2])  # Remove 'ar' (basic rule)\n",
    "        elif word.endswith('er'):\n",
    "            lemmatized_words.append(word[:-2])  # Remove 'er' (basic rule)\n",
    "        elif word.endswith('ir'):\n",
    "            lemmatized_words.append(word[:-2])  # Remove 'ir' (basic rule)\n",
    "        else:\n",
    "            # Return the word as is if no rule applies\n",
    "            lemmatized_words.append(word)\n",
    "    return lemmatized_words\n",
    "\n",
    "\n",
    "def stem_and_lemmatize(words):\n",
    "    \"\"\"Stem and Lemmatize words\"\"\"\n",
    "    stems = stem_words(words)\n",
    "    lemmas = lemmatize_verbs(words)\n",
    "    return stems + lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainData['Descripcion2'] = trainData['Descripcion'].apply(\n",
    "#     corregir_contracciones_espanol)\n",
    "# trainData['Descripcion2'] = trainData['Descripcion2'].apply(\n",
    "#     preprocessing)\n",
    "# trainData['Descripcion3'] = trainData['Descripcion2'].apply(\n",
    "#     lambda x: ' '.join(map(str, x)))\n",
    "# print(len(trainData))\n",
    "# trainData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_good_test_data(full_data= False):\n",
    "    trainData = getTrainData()\n",
    "    trainData = trainData[trainData[\"Titulo\"].notna()]\n",
    "    if full_data:\n",
    "        trainData['Descripcion'] = trainData['Titulo'] + \\\n",
    "            trainData['Descripcion']\n",
    "    trainData['Descripcion2'] = trainData['Descripcion'].apply(\n",
    "        corregir_contracciones_espanol)\n",
    "    trainData['Descripcion2'] = trainData['Descripcion2'].apply(\n",
    "        preprocessing)\n",
    "    trainData['Descripcion2'] = trainData['Descripcion2'].apply(\n",
    "        stem_and_lemmatize)\n",
    "    trainData['Descripcion3'] = trainData['Descripcion2'].apply(\n",
    "        lambda x: ' '.join(map(str, x)))\n",
    "    return trainData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "def superTrain(dataframe):\n",
    "    X = dataframe['Descripcion3']\n",
    "    y = dataframe['Label']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "    # Crear los modelos\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(),\n",
    "        \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        \"SVM\": SVC(),\n",
    "        \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "        \"Naive Bayes\": MultinomialNB()\n",
    "    }\n",
    "\n",
    "    # Resultados de los modelos\n",
    "    results = {}\n",
    "\n",
    "    # Entrenar y evaluar cada modelo\n",
    "    for name, model in models.items():\n",
    "        pipeline = make_pipeline(\n",
    "            TfidfVectorizer(),  # Vectorización de texto\n",
    "            model\n",
    "        )\n",
    "\n",
    "        # Entrenar el modelo\n",
    "        pipeline.fit(X_train, y_train)\n",
    "\n",
    "        # Predecir y evaluar el modelo\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "\n",
    "        # Guardar los resultados del reporte de clasificación\n",
    "        results[name] = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "    # Imprimir los resultados de todos los modelos\n",
    "    for model_name, report in results.items():\n",
    "        print(f\"\\nModelo: {model_name}\")\n",
    "        print(\"Clasificación detallada:\")\n",
    "        print(pd.DataFrame(report).T)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = get_good_test_data(1)\n",
    "goodTrain = superTrain(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'goodTrain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpprint\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pprint\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m keys \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgoodTrain\u001b[49m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(keys)\n\u001b[1;32m      4\u001b[0m     pprint(goodTrain[keys][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro avg\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1-score\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'goodTrain' is not defined"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "for keys in goodTrain:\n",
    "    print(keys)\n",
    "    pprint(goodTrain[keys]['macro avg']['f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.96545601291364,\n",
       "  'recall': 0.8374404928591431,\n",
       "  'f1-score': 0.8969033515783159,\n",
       "  'support': 7142.0},\n",
       " '1': {'precision': 0.8936813186813187,\n",
       "  'recall': 0.9785420635716434,\n",
       "  'f1-score': 0.9341884841813047,\n",
       "  'support': 9973.0},\n",
       " 'accuracy': 0.9196611159801343,\n",
       " 'macro avg': {'precision': 0.9295686657974793,\n",
       "  'recall': 0.9079912782153933,\n",
       "  'f1-score': 0.9155459178798103,\n",
       "  'support': 17115.0},\n",
       " 'weighted avg': {'precision': 0.9236325232508332,\n",
       "  'recall': 0.9196611159801343,\n",
       "  'f1-score': 0.9186295933223771,\n",
       "  'support': 17115.0}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goodTrain['XGBoost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def tune_models_cv(dataframe, cv_folds=5):\n",
    "    X = dataframe['Descripcion3']\n",
    "    y = dataframe['Label']\n",
    "\n",
    "    param_grid_xgb = {\n",
    "        'xgbclassifier__n_estimators': [200, 250, 300],\n",
    "        'xgbclassifier__max_depth': [7, 9, 11],\n",
    "        'xgbclassifier__learning_rate': [0.2, 0.3, 0.5]\n",
    "    }\n",
    "\n",
    "    # param_grid_rf = {\n",
    "    #     'randomforestclassifier__n_estimators': [100, 200, 300],\n",
    "    #     'randomforestclassifier__max_depth': [None, 10, 20],\n",
    "    #     'randomforestclassifier__min_samples_split': [2, 5, 10]\n",
    "    # }\n",
    "\n",
    "    models = {\n",
    "        \"XGBoost\": (XGBClassifier(eval_metric='logloss'), param_grid_xgb),\n",
    "        # \"Random Forest\": (RandomForestClassifier(random_state=42), param_grid_rf)\n",
    "    }\n",
    "\n",
    "    best_models = {}\n",
    "\n",
    "    for name, (model, param_grid) in models.items():\n",
    "        pipeline = make_pipeline(TfidfVectorizer(), model)\n",
    "        grid_search = GridSearchCV(\n",
    "            pipeline, param_grid, cv=cv_folds, scoring='f1_macro', n_jobs=-1, verbose=1\n",
    "        )\n",
    "        grid_search.fit(X, y)\n",
    "\n",
    "        best_model = grid_search.best_estimator_\n",
    "        cv_scores = cross_val_score(\n",
    "            best_model, X, y, cv=cv_folds, scoring='f1_macro', n_jobs=-1)\n",
    "\n",
    "        best_models[name] = best_model\n",
    "        print(f\"\\nMejor modelo para {name}:\")\n",
    "        print(grid_search.best_params_)\n",
    "        print(\n",
    "            f\"Mejor f1-score en validación cruzada: {grid_search.best_score_:.4f}\")\n",
    "        print(\n",
    "            f\"F1-score promedio en validación cruzada: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "\n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = get_good_test_data(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_models = tune_models_cv(trainData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mejor modelo para XGBoost:\n",
    "{'xgbclassifier__learning_rate': 0.2, 'xgbclassifier__max_depth': 7, 'xgbclassifier__n_estimators': 200}\n",
    "\n",
    "Mejor f1-score en validación cruzada: 0.9218\n",
    "\n",
    "F1-score promedio en validación cruzada: 0.9218 (+/- 0.0022)\n",
    "\n",
    "\n",
    "### Mejor modelo Random Forest\n",
    "{'randomforestclassifier__max_depth': None, 'randomforestclassifier__min_samples_split': 10, 'randomforestclassifier__n_estimators': 300}\n",
    "\n",
    "Mejor f1-score en validación cruzada: 0.8991\n",
    "\n",
    "F1-score promedio en validación cruzada: 0.8991 (+/- 0.0019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
